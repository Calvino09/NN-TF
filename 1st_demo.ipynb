{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import re\n",
    "import logging\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "import visualizations as vis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = pd.read_csv(\"all/train.csv\")\n",
    "mnist_test = pd.read_csv(\"all/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist_test)/(len(mnist_test) + len(mnist_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.4)\n",
    "for train_index, validate_index in split.split(mnist_train, mnist_train['label']):\n",
    "    train_data = mnist_train.loc[train_index]\n",
    "    validate_data = mnist_train.loc[validate_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mnist_train.head()\n",
    "train_label = train_data['label']\n",
    "train_data.drop('label', axis=1, inplace = True)\n",
    "train_label.reset_index(drop=True, inplace=True)\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "validate_label = validate_data['label']\n",
    "validate_data.drop('label', axis=1, inplace = True)\n",
    "validate_label.reset_index(drop=True, inplace=True)\n",
    "validate_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train = np.concatenate([train_data, validate_data])\n",
    "all_label = np.concatenate([train_label, validate_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25200, 784), (25200,), (42000, 784), (42000,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, train_label.shape, all_train.shape, all_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = np.eye(10)[train_label[:100]]\n",
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test.head()\n",
    "mnist_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. one-hot sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "train_images = train_images / 255.0\n",
    "\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#model.fit(np.array(train_data), np.array(train_label), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.4759 - acc: 0.8300\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 3s 58us/step - loss: 0.3601 - acc: 0.8684\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.3235 - acc: 0.8808\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.3014 - acc: 0.8880\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.2799 - acc: 0.8961\n",
      "10000/10000 [==============================] - 0s 47us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.34023452818393707, 0.8787]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=5)\n",
    "\n",
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "42000/42000 [==============================] - 3s 83us/step - loss: 14.5585 - acc: 0.0968\n",
      "Epoch 2/5\n",
      "42000/42000 [==============================] - 3s 81us/step - loss: 14.5585 - acc: 0.0968\n",
      "Epoch 3/5\n",
      "42000/42000 [==============================] - 3s 82us/step - loss: 14.5585 - acc: 0.0968\n",
      "Epoch 4/5\n",
      "42000/42000 [==============================] - 4s 84us/step - loss: 14.5585 - acc: 0.0968\n",
      "Epoch 5/5\n",
      "42000/42000 [==============================] - 4s 84us/step - loss: 14.5585 - acc: 0.0968\n",
      "16800/16800 [==============================] - 1s 33us/step\n",
      "Test accuracy: 0.09678571428571428\n"
     ]
    }
   ],
   "source": [
    "model.fit(np.array(all_train), np.array(all_label), epochs=5)\n",
    "test_loss, test_acc = model.evaluate(validate_data, validate_label)\n",
    "\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model， 1-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHY 10 DIMENSITON? -> 0-9 recognition\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "#tf.truncated_normal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lazy evaluation\n",
    "#cross_entropy = -tf.reduce_sum(y*tf.log(pred))\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "\n",
    "#xentropy = tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = pred)\n",
    "#loss = tf.reduce_mean(xentropy, name='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= nan\n",
      "Epoch: 0002 cost= nan\n",
      "Epoch: 0003 cost= nan\n",
      "Epoch: 0004 cost= nan\n",
      "Epoch: 0005 cost= nan\n",
      "Epoch: 0006 cost= nan\n",
      "Epoch: 0007 cost= nan\n",
      "Epoch: 0008 cost= nan\n",
      "Epoch: 0009 cost= nan\n",
      "Epoch: 0010 cost= nan\n",
      "Epoch: 0011 cost= nan\n",
      "Epoch: 0012 cost= nan\n",
      "Epoch: 0013 cost= nan\n",
      "Epoch: 0014 cost= nan\n",
      "Epoch: 0015 cost= nan\n",
      "Epoch: 0016 cost= nan\n",
      "Epoch: 0017 cost= nan\n",
      "Epoch: 0018 cost= nan\n",
      "Epoch: 0019 cost= nan\n",
      "Epoch: 0020 cost= nan\n",
      "Epoch: 0021 cost= nan\n",
      "Epoch: 0022 cost= nan\n",
      "Epoch: 0023 cost= nan\n",
      "Epoch: 0024 cost= nan\n",
      "Epoch: 0025 cost= nan\n",
      "Optimization Finished!\n",
      "Tensor(\"Equal_1:0\", shape=(?,), dtype=bool)\n",
      "Accuracy: 0.09839286\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch  = int(len(train_data)/batch_size)\n",
    "\n",
    "        for iteration in range(total_batch):\n",
    "            x_batch = np.array(all_train[iteration * batch_size : min((iteration+1) * batch_size, len(train_data))])\n",
    "            y_batch = np.array(all_label[iteration * batch_size : min((iteration+1) * batch_size, len(train_label))])\n",
    "            \n",
    "            _, c = sess.run([optimizer, cost], feed_dict = {x: x_batch, y: np.eye(10)[y_batch]})\n",
    "            #print(c)\n",
    "            avg_cost += c / total_batch\n",
    "        \n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "    \n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    print(correct_prediction)\n",
    "    \n",
    "    # Calculate accuracy for 3000 examples\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: np.array(validate_data), y: np.eye(10)[validate_label]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model2: 2-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_model_on_train_set(n_neurons_1=300, n_neurons_2 = 100, learning_rate = 0.01, n_epochs = 30, batch_size = 50):\n",
    "    # here we build a two layers NN model and test on validation set, you may improve it to a CV version\n",
    "    # n_neurons_1 : number of neurons in the first layer\n",
    "    # n_neurons_2  : number of neurons in the second layer\n",
    "    # learning_rate : the learning rate of BGD\n",
    "    # n_epochs : times of training the model\n",
    "    # batch_size : since we adopted BGD, then we need to define the size of a size\n",
    "    # initialize variables\n",
    "    X = tf.placeholder(tf.float32, shape=(None, 28*28), name='X')\n",
    "    y = tf.placeholder(tf.int64, shape=(None), name = 'y')\n",
    "\n",
    "    # weights\n",
    "    W1 = tf.Variable(tf.truncated_normal((28*28, n_neurons_1),stddev = 0.01), name = 'layer_1')\n",
    "    W2 = tf.Variable(tf.truncated_normal((n_neurons_1, n_neurons_2),stddev = 0.01), name = 'layer_2')\n",
    "    W3 = tf.Variable(tf.truncated_normal((n_neurons_2 , 10),stddev = 0.01), name = 'output_layer')\n",
    "\n",
    "    # biases\n",
    "    b1 = tf.Variable(tf.zeros([n_neurons_1]), name='b_1')\n",
    "    b2 = tf.Variable(tf.zeros([n_neurons_2]), name='b_2')\n",
    "    b3 = tf.Variable(tf.zeros([10]), name='b_3')\n",
    "\n",
    "    # the output of each layer\n",
    "    Z1 = tf.nn.relu(tf.matmul(X,W1) + b1)\n",
    "    Z2 = tf.nn.relu(tf.matmul(Z1, W2) + b2)\n",
    "    output = tf.matmul(Z2, W3) + b3\n",
    "\n",
    "    # define loss function. Cross-entropy was adopted rather than MSE\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = output)\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "    # define accuracy\n",
    "    correct = tf.nn.in_top_k(output, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    # run everything\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            for iteration in range(len(train_data) // batch_size):\n",
    "                # 因为要batchSize=50个50个的取，取到末尾时可能不够，所以用个if判定一下\n",
    "                if (iteration + 1) * batch_size <= len(train_data):\n",
    "                    X_batch = np.array(train_data[iteration * batch_size : iteration * batch_size + batch_size])\n",
    "                    y_batch = np.array(train_label[iteration * batch_size : iteration * batch_size + batch_size])\n",
    "                else:\n",
    "                    X_batch = np.array(train_data[iteration * batch_size : ])\n",
    "                    y_batch = np.array(train_label[iteration * batch_size : ])\n",
    "                sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "            # train error\n",
    "            acc_train = accuracy.eval(feed_dict={X:X_batch, y:y_batch})\n",
    "            # test error\n",
    "            acc_test = accuracy.eval(feed_dict={X:np.array(validate_data),\n",
    "                                               y:np.array(validate_label)})\n",
    "            print(epoch, 'Train accuracy:', acc_train, 'Test accuracy:', acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_model_to_predict(n_neurons_1=300, n_neurons_2 = 100, learning_rate = 0.01, n_epochs = 30, batch_size = 50):\n",
    "    # here we build a two layers NN model and test on validation set, you may improve it to a CV version\n",
    "    # n_neurons_1 : number of neurons in the first layer\n",
    "    # n_neurons_2  : number of neurons in the second layer\n",
    "    # learning_rate : the learning rate of BGD\n",
    "    # n_epochs : times of training the model\n",
    "    # batch_size : since we adopted BGD, then we need to define the size of a size\n",
    "    # initialize variables\n",
    "    X = tf.placeholder(tf.float32, shape=(None, 28*28), name='X')\n",
    "    y = tf.placeholder(tf.int64, shape=(None), name = 'y')\n",
    "\n",
    "    # weights\n",
    "    W1 = tf.Variable(tf.truncated_normal((28*28, n_neurons_1),stddev = 0.01), name = 'layer_1')\n",
    "    W2 = tf.Variable(tf.truncated_normal((n_neurons_1, n_neurons_2),stddev = 0.01), name = 'layer_2')\n",
    "    W3 = tf.Variable(tf.truncated_normal((n_neurons_2 , 10),stddev = 0.01), name = 'output_layer')\n",
    "\n",
    "    # biases\n",
    "    b1 = tf.Variable(tf.zeros([n_neurons_1]), name='b_1')\n",
    "    b2 = tf.Variable(tf.zeros([n_neurons_2]), name='b_2')\n",
    "    b3 = tf.Variable(tf.zeros([10]), name='b_3')\n",
    "\n",
    "    # the output of each layer\n",
    "    Z1 = tf.nn.relu(tf.matmul(X,W1) + b1)\n",
    "    Z2 = tf.nn.relu(tf.matmul(Z1, W2) + b2)\n",
    "    output = tf.matmul(Z2, W3) + b3\n",
    "\n",
    "    # define loss function. Cross-entropy was adopted rather than MSE\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = output)\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "    # define accuracy\n",
    "    correct = tf.nn.in_top_k(output, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            for iteration in range(len(train_data) // batch_size):\n",
    "                X_batch = np.array(all_train[iteration * batch_size : min((iteration+1) * batch_size, len(train_data))])\n",
    "                y_batch = np.array(all_label[iteration * batch_size : min((iteration+1) * batch_size, len(train_label))])\n",
    "                '''if (iteration + 1) * batch_size <= len(train_data):\n",
    "                    X_batch = np.array(train_data[iteration * batch_size : iteration * batch_size + batch_size])\n",
    "                    y_batch = np.array(train_label[iteration * batch_size : iteration * batch_size + batch_size])\n",
    "                else:\n",
    "                    X_batch = np.array(train_data[iteration * batch_size : ])\n",
    "                    y_batch = np.array(train_label[iteration * batch_size : ])'''\n",
    "                sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "            acc_train = accuracy.eval(feed_dict={X:X_batch, y:y_batch})\n",
    "            # test error\n",
    "            acc_test = accuracy.eval(feed_dict={X:np.array(validate_data),\n",
    "                                               y:np.array(validate_label)})\n",
    "            print(epoch, 'Train accuracy:', acc_train, 'Test accuracy:', acc_test)\n",
    "        \n",
    "        predict_output = sess.run(output,feed_dict={X:np.array(mnist_test)})\n",
    "        return np.argmax(predict_output, axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.98 Test accuracy: 0.945\n",
      "1 Train accuracy: 1.0 Test accuracy: 0.95839286\n",
      "2 Train accuracy: 1.0 Test accuracy: 0.96369046\n",
      "3 Train accuracy: 1.0 Test accuracy: 0.9649405\n",
      "4 Train accuracy: 1.0 Test accuracy: 0.96416664\n",
      "5 Train accuracy: 1.0 Test accuracy: 0.9670238\n",
      "6 Train accuracy: 1.0 Test accuracy: 0.9652381\n",
      "7 Train accuracy: 1.0 Test accuracy: 0.9704762\n",
      "8 Train accuracy: 1.0 Test accuracy: 0.9681548\n",
      "9 Train accuracy: 1.0 Test accuracy: 0.96410716\n",
      "10 Train accuracy: 1.0 Test accuracy: 0.97059524\n",
      "11 Train accuracy: 1.0 Test accuracy: 0.97125\n",
      "12 Train accuracy: 1.0 Test accuracy: 0.97369045\n",
      "13 Train accuracy: 1.0 Test accuracy: 0.9751786\n",
      "14 Train accuracy: 1.0 Test accuracy: 0.9755357\n",
      "15 Train accuracy: 1.0 Test accuracy: 0.9751786\n",
      "16 Train accuracy: 1.0 Test accuracy: 0.9750595\n",
      "17 Train accuracy: 1.0 Test accuracy: 0.9751786\n",
      "18 Train accuracy: 1.0 Test accuracy: 0.9753571\n",
      "19 Train accuracy: 1.0 Test accuracy: 0.9753571\n",
      "20 Train accuracy: 1.0 Test accuracy: 0.9755357\n",
      "21 Train accuracy: 1.0 Test accuracy: 0.97529763\n",
      "22 Train accuracy: 1.0 Test accuracy: 0.9752381\n",
      "23 Train accuracy: 1.0 Test accuracy: 0.97529763\n",
      "24 Train accuracy: 1.0 Test accuracy: 0.97529763\n",
      "25 Train accuracy: 1.0 Test accuracy: 0.97529763\n",
      "26 Train accuracy: 1.0 Test accuracy: 0.9752381\n",
      "27 Train accuracy: 1.0 Test accuracy: 0.9752381\n",
      "28 Train accuracy: 1.0 Test accuracy: 0.97511905\n",
      "29 Train accuracy: 1.0 Test accuracy: 0.9751786\n",
      "30 Train accuracy: 1.0 Test accuracy: 0.9752381\n",
      "31 Train accuracy: 1.0 Test accuracy: 0.9752381\n",
      "32 Train accuracy: 1.0 Test accuracy: 0.9751786\n",
      "33 Train accuracy: 1.0 Test accuracy: 0.9751786\n",
      "34 Train accuracy: 1.0 Test accuracy: 0.9751786\n",
      "35 Train accuracy: 1.0 Test accuracy: 0.9750595\n",
      "36 Train accuracy: 1.0 Test accuracy: 0.9750595\n",
      "37 Train accuracy: 1.0 Test accuracy: 0.9750595\n",
      "38 Train accuracy: 1.0 Test accuracy: 0.97511905\n",
      "39 Train accuracy: 1.0 Test accuracy: 0.9751786\n",
      "40 Train accuracy: 1.0 Test accuracy: 0.97511905\n",
      "41 Train accuracy: 1.0 Test accuracy: 0.9750595\n",
      "42 Train accuracy: 1.0 Test accuracy: 0.9750595\n",
      "43 Train accuracy: 1.0 Test accuracy: 0.975\n",
      "44 Train accuracy: 1.0 Test accuracy: 0.975\n",
      "45 Train accuracy: 1.0 Test accuracy: 0.97488093\n",
      "46 Train accuracy: 1.0 Test accuracy: 0.97488093\n",
      "47 Train accuracy: 1.0 Test accuracy: 0.97488093\n",
      "48 Train accuracy: 1.0 Test accuracy: 0.97482145\n",
      "49 Train accuracy: 1.0 Test accuracy: 0.97482145\n"
     ]
    }
   ],
   "source": [
    "prediction = NN_model_to_predict(n_neurons_1=300, n_neurons_2 = 100, learning_rate = 0.01, n_epochs = 50, batch_size = 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'ImageId': [i for i in range(1,len(prediction)+1)],\n",
    "                  'Label': prediction})\n",
    "df.to_csv('./my_prediction.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: 5-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def five_NN_model_to_predict(learning_rate = 0.001, n_epochs = 2000, batch_size = 100):\n",
    "    # here we build a two layers NN model and test on validation set, you may improve it to a CV version\n",
    "    # n_neurons_1 : number of neurons in the first layer\n",
    "    # n_neurons_2  : number of neurons in the second layer\n",
    "    # learning_rate : the learning rate of BGD\n",
    "    # n_epochs : times of training the model\n",
    "    # batch_size : since we adopted BGD, then we need to define the size of a size\n",
    "    # initialize variables\n",
    "    X = tf.placeholder(tf.float32, shape=(None, 28*28), name='X')\n",
    "    y = tf.placeholder(tf.int64, shape=(None), name = 'y')\n",
    "\n",
    "    \n",
    "    # layer size\n",
    "    \n",
    "    L1 = 300\n",
    "    L2 = 120\n",
    "    L3 = 60\n",
    "    L4 = 30\n",
    "    L5 = 10\n",
    "    \n",
    "    # weights\n",
    "    W1 = tf.Variable(tf.truncated_normal((28*28, L1),stddev = 0.01), name = 'layer_1')\n",
    "    W2 = tf.Variable(tf.truncated_normal((L1, L2),stddev = 0.01), name = 'layer_2')\n",
    "    W3 = tf.Variable(tf.truncated_normal((L2, L3),stddev = 0.01), name = 'layer_3')\n",
    "    W4 = tf.Variable(tf.truncated_normal((L3, L4),stddev = 0.01), name = 'layer_4')\n",
    "    W5 = tf.Variable(tf.truncated_normal((L4 , L5),stddev = 0.01), name = 'output_layer')\n",
    "\n",
    "    # biases\n",
    "    b1 = tf.Variable(tf.zeros([L1]), name='b_1')\n",
    "    b2 = tf.Variable(tf.zeros([L2]), name='b_2')\n",
    "    b3 = tf.Variable(tf.zeros([L3]), name='b_3')\n",
    "    b4 = tf.Variable(tf.zeros([L4]), name='b_4')\n",
    "    b5 = tf.Variable(tf.zeros([L5]), name='b_3')\n",
    "\n",
    "    # the output of each layer\n",
    "    Z1 = tf.nn.relu(tf.matmul(X,W1) + b1)\n",
    "    Z2 = tf.nn.relu(tf.matmul(Z1, W2) + b2)\n",
    "    Z3 = tf.nn.relu(tf.matmul(Z2, W3) + b3)\n",
    "    Z4 = tf.nn.relu(tf.matmul(Z3, W4) + b4)\n",
    "    \n",
    "    output = tf.matmul(Z4, W5) + b5\n",
    "\n",
    "    # define loss function. Cross-entropy was adopted rather than MSE\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = output)\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')*100\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    #training_op = optimizer.minimize(loss)\n",
    "    training_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    # define accuracy\n",
    "    correct = tf.nn.in_top_k(output, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            for iteration in range(len(train_data) // batch_size):\n",
    "                X_batch = np.array(all_train[iteration * batch_size : min((iteration+1) * batch_size, len(train_data))])\n",
    "                y_batch = np.array(all_label[iteration * batch_size : min((iteration+1) * batch_size, len(train_label))])\n",
    "                '''if (iteration + 1) * batch_size <= len(train_data):\n",
    "                    X_batch = np.array(train_data[iteration * batch_size : iteration * batch_size + batch_size])\n",
    "                    y_batch = np.array(train_label[iteration * batch_size : iteration * batch_size + batch_size])\n",
    "                else:\n",
    "                    X_batch = np.array(train_data[iteration * batch_size : ])\n",
    "                    y_batch = np.array(train_label[iteration * batch_size : ])'''\n",
    "                sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "            acc_train = accuracy.eval(feed_dict={X:X_batch, y:y_batch})\n",
    "            # test error\n",
    "            acc_test = accuracy.eval(feed_dict={X:np.array(validate_data),\n",
    "                                               y:np.array(validate_label)})\n",
    "            print(epoch, 'Train accuracy:', acc_train, 'Test accuracy:', acc_test)\n",
    "        \n",
    "        predict_output = sess.run(output,feed_dict={X:np.array(mnist_test)})\n",
    "        return np.argmax(predict_output, axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = five_NN_model_to_predict()\n",
    "df = pd.DataFrame({'ImageId': [i for i in range(1,len(prediction)+1)],\n",
    "                  'Label': prediction})\n",
    "df.to_csv('./my_prediction.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4. CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(learning_rate = 0.001, n_epochs = 50, batch_size = 100):\n",
    "    # what args do we need ? - -|\n",
    "    #NUM_ITERS=5000\n",
    "    #DISPLAY_STEP=100\n",
    "    #BATCH=100\n",
    "    \n",
    "    #\n",
    "    # input layer               - X[batch, 28, 28]\n",
    "    # 1 conv. layer             - W1[5, 5, 1, C1] + b1[C1]   pad = 2?\n",
    "    #                             Y1[batch, 28, 28, C1]\n",
    "    # 2 conv. layer             - W2[3, 3, C1, C2] + b2[C2]\n",
    "    # 2.1 max pooling filter 2x2, stride 2 - down sample the input (rescale input by 2) 28x28-> 14x14\n",
    "    #                             Y2[batch, 14,14,C2] \n",
    "    # 3 conv. layer             - W3[3, 3, C2, C3]  + b3[C3]\n",
    "    # 3.1 max pooling filter 2x2, stride 2 - down sample the input (rescale input by 2) 14x14-> 7x7\n",
    "    #                             Y3[batch, 7, 7, C3] \n",
    "    # 4 fully connecteed layer  - W4[7*7*C3, FC4]   + b4[FC4]\n",
    "    #                             Y4[batch, FC4] \n",
    "    # 5 output layer            - W5[FC4, 10]   + b5[10]\n",
    "    # One-hot encoded labels      Y5[batch, 10]\n",
    "    \n",
    "    # input \n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape=(None, 28, 28, 1), name='X')\n",
    "    y = tf.placeholder(tf.int64, shape=(None), name = 'y')\n",
    "    \n",
    "    # Probability of keeping a node during dropout = 1.0 at test time (no dropout) and 0.75 at training time\n",
    "    pkeep = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # layer size, for cnn is conv depth (the number of detector)\n",
    "    \n",
    "    C1 = 4\n",
    "    C2 = 8\n",
    "    C3 = 16\n",
    "    \n",
    "    FC4 = 256  # fully connected layer\n",
    "    \n",
    "    stride = 1\n",
    "    k = 2\n",
    "    # conv 1\n",
    "    W1 = tf.Variable(tf.truncated_normal((5,5, 1, C1),stddev = 0.01), name = 'conv_1')\n",
    "    b1 = tf.Variable(tf.truncated_normal([C1], stddev = 0.01))\n",
    "    \n",
    "    Y1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides=[1, stride, stride, 1], padding=\"SAME\") + b1)   \n",
    "    # conv 2 + maxpooling\n",
    "    W2 = tf.Variable(tf.truncated_normal((3,3, C1, C2),stddev = 0.01), name = 'conv_2')\n",
    "    b2 = tf.Variable(tf.truncated_normal([C2], stddev = 0.01))\n",
    "    \n",
    "    Y2 = tf.nn.relu(tf.nn.conv2d(Y1, W2, strides=[1, stride, stride, 1], padding=\"SAME\") + b2)   \n",
    "    Y2 = tf.nn.max_pool(Y2, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding=\"SAME\")\n",
    "    \n",
    "    # conv3 + maxpooling\n",
    "    W3 = tf.Variable(tf.truncated_normal((3,3, C2, C3),stddev = 0.01), name = 'conv_3')\n",
    "    b3 = tf.Variable(tf.truncated_normal([C3], stddev = 0.01))\n",
    "    \n",
    "    Y3 = tf.nn.relu(tf.nn.conv2d(Y2, W3, strides=[1, stride, stride, 1], padding=\"SAME\") + b3)   \n",
    "    Y3 = tf.nn.max_pool(Y3, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding=\"SAME\")\n",
    "    \n",
    "    # full connected \n",
    "    YY = tf.reshape(Y3, shape=[-1, 7 * 7 * C3])\n",
    "    \n",
    "    W4 = tf.Variable(tf.truncated_normal([7*7*C3, FC4], stddev=0.01, name = \"full_connected\"))\n",
    "    b4 = tf.Variable(tf.truncated_normal([FC4], stddev=0.01))\n",
    "    \n",
    "    Y4 = tf.nn.relu(tf.matmul(YY, W4) + b4)\n",
    "    \n",
    "    # calculate softmax mapping to 10 classification\n",
    "    W5 = tf.Variable(tf.truncated_normal([FC4, 10], stddev=0.01))\n",
    "    b5 = tf.Variable(tf.truncated_normal([10], stddev=0.01))\n",
    "    \n",
    "    Y5 = tf.nn.relu(tf.matmul(Y4, W5) + b5)\n",
    "    \n",
    "    Y = tf.nn.softmax(Y5)\n",
    "    \n",
    "    # loss function\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=Y, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy) * 100\n",
    "    \n",
    "    # optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    #training_op = optimizer.minimize(loss)\n",
    "    training_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    # accuracy\n",
    "    correct = tf.nn.in_top_k(Y, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "    #correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(y, 1))\n",
    "    #accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # matplotlib visualization\n",
    "    allweights = tf.concat([tf.reshape(W1, [-1]), tf.reshape(W2, [-1]), tf.reshape(W3, [-1]), tf.reshape(W4, [-1]), tf.reshape(W5, [-1])], 0)\n",
    "    allbiases  = tf.concat([tf.reshape(b1, [-1]), tf.reshape(b2, [-1]), tf.reshape(b3, [-1]), tf.reshape(b4, [-1]), tf.reshape(b5, [-1])], 0)\n",
    "\n",
    "    \n",
    "    # init \n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    train_losses = list()\n",
    "    train_acc = list()\n",
    "    test_losses = list()\n",
    "    test_acc = list()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # run session\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            for iteration in range(len(train_data) // batch_size):\n",
    "                X_batch = np.array(all_train[iteration * batch_size : min((iteration+1) * batch_size, len(train_data))])\n",
    "                y_batch = np.array(all_label[iteration * batch_size : min((iteration+1) * batch_size, len(train_label))])\n",
    "                \n",
    "                sess.run(training_op, feed_dict={X:np.reshape(X_batch, (len(X_batch), 28, 28, 1)), y:y_batch, pkeep: 0.85})\n",
    "            \n",
    "            acc_trn, loss_trn, w, b = sess.run([accuracy, loss, allweights, allbiases], feed_dict={X:np.reshape(X_batch, (len(X_batch), 28, 28, 1)), y:y_batch, pkeep: 1.0})\n",
    "            \n",
    "            acc_tst, loss_tst = sess.run([accuracy, loss], feed_dict={X:np.reshape(np.array(validate_data), (len(validate_data), 28, 28, 1)),\n",
    "                                               y:np.array(validate_label), pkeep: 1.0})\n",
    "            \n",
    "            print(\"#{} Trn acc={} , Trn loss={} Tst acc={} , Tst loss={}\".format(epoch,acc_trn,loss_trn,acc_tst,loss_tst))\n",
    "\n",
    "            train_losses.append(loss_trn)\n",
    "            train_acc.append(acc_trn)\n",
    "            test_losses.append(loss_tst)\n",
    "            test_acc.append(acc_tst)\n",
    "\n",
    "    title = \"MNIST_3.0 5 layers 3 conv\"\n",
    "    vis.losses_accuracies_plots(train_losses,train_acc,test_losses, test_acc,title,n_epochs)\n",
    "       \n",
    "    predict_output = sess.run(Y,feed_dict={X:X:np.reshape(np.array(mnist_test), (len(mnist_test), 28, 28, 1))})\n",
    "        \n",
    "    return np.argmax(predict_output, axis= 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 Trn acc=0.8500000238418579 , Trn loss=198.92689514160156 Tst acc=0.8520833253860474 , Tst loss=200.074462890625\n",
      "#1 Trn acc=0.949999988079071 , Trn loss=198.0220947265625 Tst acc=0.928511917591095 , Tst loss=199.3626708984375\n",
      "#2 Trn acc=0.9800000190734863 , Trn loss=197.552978515625 Tst acc=0.9424999952316284 , Tst loss=199.33871459960938\n",
      "#3 Trn acc=0.9800000190734863 , Trn loss=197.6280975341797 Tst acc=0.9701785445213318 , Tst loss=199.19215393066406\n",
      "#4 Trn acc=0.9900000095367432 , Trn loss=197.49407958984375 Tst acc=0.9549404978752136 , Tst loss=198.53636169433594\n",
      "#5 Trn acc=0.9700000286102295 , Trn loss=197.83547973632812 Tst acc=0.9612500071525574 , Tst loss=198.80552673339844\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-0a8ead4c6381>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_model\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-64-331c586275d9>\u001b[0m in \u001b[0;36mcnn_model\u001b[0;34m(learning_rate, n_epochs, batch_size)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             acc_tst, loss_tst = sess.run([accuracy, loss], feed_dict={X:np.reshape(np.array(validate_data), (len(validate_data), 28, 28, 1)),\n\u001b[0;32m--> 120\u001b[0;31m                                                y:np.array(validate_label), pkeep: 1.0})\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"#{} Trn acc={} , Trn loss={} Tst acc={} , Tst loss={}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc_trn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_trn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc_tst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_tst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prediction = cnn_model( n_epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
