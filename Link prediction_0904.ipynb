{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuqian/.pyenv/versions/3.6.5/envs/env3.6/lib/python3.6/site-packages/pandas/compat/__init__.py:84: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/home/yuqian/.pyenv/versions/3.6.5/envs/env3.6/lib/python3.6/site-packages/pandas/compat/__init__.py:84: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "import gensim\n",
    "import os\n",
    "import collections\n",
    "import smart_open\n",
    "import random\n",
    "\n",
    "import networkx as nx \n",
    "import turicreate as tc\n",
    "import tqdm\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Load Original network ...+ Relation Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "2222\n",
      "44412\n"
     ]
    }
   ],
   "source": [
    "QuestionList = []\n",
    "EmbeddingList = []\n",
    "\n",
    "# Read node2vec representation\n",
    "with open(\"../data/vec_all_node2vec.txt\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "        QuestionId = int(line.split()[0])\n",
    "        QuestionList.append(QuestionId)\n",
    "        #EmbeddingList.append(line.split()[1:])\n",
    "        EmbeddingList.append([float(x) for x in line.split()[1:]])\n",
    "        #print(EmbeddingList[0])\n",
    "        #Embedding = line.split()[1:]\n",
    "        #print(QuestionId, Embedding)\n",
    "embedding = np.asarray(EmbeddingList)\n",
    "\n",
    "# Read-in tri_dnr representation\n",
    "tri_dnr = []\n",
    "with open(\"../data/triDNR_rep.txt\",\"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "        QuestionId = int(line.split()[0])\n",
    "        #val_list.append(QuestionId)\n",
    "        tri_dnr.append([float(x) for x in line.split()[1:]])\n",
    "        #Embedding = line.split()[1:]\n",
    "        #print(QuestionId, Embedding)\n",
    "tri_dnr = np.asarray(tri_dnr)\n",
    "\n",
    "# Read-in text data\n",
    "def read_in_text_data(para, QuestionList):\n",
    "    data_list = [\"\"] * len(QuestionList)\n",
    "    if para == \"body\":\n",
    "        with open(\"../src/Body_data.txt\",\"r\") as fp:\n",
    "            lines = fp.read()\n",
    "    if para == \"title\":\n",
    "        with open(\"../src/Title_data.txt\",\"r\") as fp:\n",
    "            lines = fp.read()\n",
    "    if para == \"tag\":\n",
    "        with open(\"../src/Tags_data.txt\",\"r\") as fp:\n",
    "            lines = fp.read()\n",
    "    line_list = lines.split(\"!!!---!!!-------------------------------------------------------------\\n\")\n",
    "    data_list = [\"\"] * len(QuestionList)\n",
    "    line_list.pop(0)\n",
    "    for data in line_list:\n",
    "        questionId = int(data.split(\"\\n\")[0])\n",
    "        question_index = QuestionList.index(questionId)\n",
    "        data_list[question_index] = data.split(\"\\n\")[1][1:-1]\n",
    "        \n",
    "    return data_list\n",
    "\n",
    "titleList = read_in_text_data(\"title\", QuestionList)\n",
    "tagList = read_in_text_data(\"tag\", QuestionList)\n",
    "\n",
    "\n",
    "# Read-in duplication pairs\n",
    "rows = []\n",
    "with open(\"../data/duplicat_pairs.txt\", \"r\") as fp:\n",
    "    for i, line in enumerate(fp):\n",
    "        Q_1 = int(line.split()[0])\n",
    "        Q_2 = int(line.split()[1])\n",
    "        rows.append((Q_1, Q_2))\n",
    "postId_list = [x[0] for x in rows]\n",
    "matched_list = [x[1] for x in rows]\n",
    "coverd = set(QuestionList) & set(postId_list)\n",
    "coverd_match = []\n",
    "for i in coverd:\n",
    "    index = postId_list.index(i)\n",
    "    match = matched_list[index]\n",
    "    if match in QuestionList:\n",
    "        coverd_match.append((i,match))\n",
    "print(len(coverd_match))\n",
    "print(len(QuestionList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint, pickle\n",
    "\n",
    "def generating_title_text_features(method=\"TF-IDF\"):\n",
    "    if method == \"d2c\":\n",
    "        with open(\"../data/title_d2v.pkl\",'rb') as f:\n",
    "            vec = pickle.load(f)\n",
    "    elif method == \"w2c\":\n",
    "        with open(\"../data/title_w2v.pkl\",'rb') as f:\n",
    "            vec = pickle.load(f)\n",
    "    elif method == \"TF-IDF\":\n",
    "        with open(\"../data/pca_tfidf.pkl\",'rb') as f:\n",
    "            vec = pickle.load(f)\n",
    "    else:\n",
    "        raise ValueError('Not defiend method: {}'.format(method))\n",
    "\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sample True/False pairwise link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_feature(feature1, feature2):\n",
    "    f1 = np.concatenate((feature1,feature2))\n",
    "    f2 = feature1 + feature2\n",
    "    f3 = feature1 - feature2\n",
    "    f4 = feature1 *feature2\n",
    "    return np.concatenate([f1,f2,f3,f4], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k : negative sampling ratio, feature_method for selecting node representation\n",
    "import random\n",
    "def generate_data(k=5, feature_method=\"triDnr\"):\n",
    "    if feature_method == \"triDnr\":\n",
    "        features = tri_dnr\n",
    "    elif feature_method == \"node2vec\":\n",
    "        features = embedding\n",
    "    else:\n",
    "        features = generating_text_features(feature_method)\n",
    "    \n",
    "    features = np.asanyarray(features)\n",
    "    #postive_sample_index = [QuestionList.index(index[0]) for index in coverd_match]\n",
    "    #postive_sample = list(features[postive_sample_index])\n",
    "    postive_sample = list()\n",
    "    negative_sample = list()\n",
    "    \n",
    "    for index in coverd_match:\n",
    "        origin = index[0]\n",
    "        duplication = index[1]\n",
    "        \n",
    "        original_feature = features[QuestionList.index(origin)]\n",
    "        duplication_feature = features[QuestionList.index(duplication)]\n",
    "        postive_sample.append(generating_feature(original_feature,duplication_feature))\n",
    "        \n",
    "        # select K negative sample (by question Id)\n",
    "        false_nodes = set()\n",
    "        \n",
    "        while len(false_nodes) < k:\n",
    "            false_node = random.choice(QuestionList)\n",
    "            if false_node in false_nodes or false_node == duplication:\n",
    "                continue\n",
    "            else:\n",
    "                false_nodes.add(false_node)\n",
    "                                \n",
    "        for node in false_nodes:\n",
    "            node_feature = features[QuestionList.index(node)]\n",
    "            negative_sample.append(generating_feature(original_feature, node_feature))\n",
    "    \n",
    "    #print(len(postive_sample[0]))\n",
    "    postive_df = pd.DataFrame(data=postive_sample)\n",
    "    postive_df['label'] = 1\n",
    "    \n",
    "    negative_df = pd.DataFrame(data=negative_sample)\n",
    "    negative_df['label'] = 0\n",
    "    \n",
    "    return postive_df.append(negative_df).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>13327</td>\n",
       "      <td>-0.048329</td>\n",
       "      <td>0.204892</td>\n",
       "      <td>-0.398776</td>\n",
       "      <td>-0.345779</td>\n",
       "      <td>-0.246285</td>\n",
       "      <td>-0.92692</td>\n",
       "      <td>0.307822</td>\n",
       "      <td>0.056094</td>\n",
       "      <td>-0.356226</td>\n",
       "      <td>-0.51482</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015278</td>\n",
       "      <td>0.111212</td>\n",
       "      <td>-0.080474</td>\n",
       "      <td>-0.012642</td>\n",
       "      <td>-0.070426</td>\n",
       "      <td>0.017351</td>\n",
       "      <td>0.011926</td>\n",
       "      <td>0.162779</td>\n",
       "      <td>-0.060195</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13328</td>\n",
       "      <td>-0.048329</td>\n",
       "      <td>0.204892</td>\n",
       "      <td>-0.398776</td>\n",
       "      <td>-0.345779</td>\n",
       "      <td>-0.246285</td>\n",
       "      <td>-0.92692</td>\n",
       "      <td>0.307822</td>\n",
       "      <td>0.056094</td>\n",
       "      <td>-0.356226</td>\n",
       "      <td>-0.51482</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027648</td>\n",
       "      <td>0.206616</td>\n",
       "      <td>0.220193</td>\n",
       "      <td>-0.081247</td>\n",
       "      <td>0.705522</td>\n",
       "      <td>0.022888</td>\n",
       "      <td>-0.010088</td>\n",
       "      <td>-0.096915</td>\n",
       "      <td>-0.005339</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13329</td>\n",
       "      <td>-0.048329</td>\n",
       "      <td>0.204892</td>\n",
       "      <td>-0.398776</td>\n",
       "      <td>-0.345779</td>\n",
       "      <td>-0.246285</td>\n",
       "      <td>-0.92692</td>\n",
       "      <td>0.307822</td>\n",
       "      <td>0.056094</td>\n",
       "      <td>-0.356226</td>\n",
       "      <td>-0.51482</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043687</td>\n",
       "      <td>0.183477</td>\n",
       "      <td>0.089562</td>\n",
       "      <td>-0.046716</td>\n",
       "      <td>-0.008383</td>\n",
       "      <td>0.064136</td>\n",
       "      <td>0.028604</td>\n",
       "      <td>0.106633</td>\n",
       "      <td>0.091731</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13330</td>\n",
       "      <td>-0.048329</td>\n",
       "      <td>0.204892</td>\n",
       "      <td>-0.398776</td>\n",
       "      <td>-0.345779</td>\n",
       "      <td>-0.246285</td>\n",
       "      <td>-0.92692</td>\n",
       "      <td>0.307822</td>\n",
       "      <td>0.056094</td>\n",
       "      <td>-0.356226</td>\n",
       "      <td>-0.51482</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025514</td>\n",
       "      <td>0.082782</td>\n",
       "      <td>-0.084981</td>\n",
       "      <td>-0.113158</td>\n",
       "      <td>-0.316173</td>\n",
       "      <td>0.123388</td>\n",
       "      <td>0.005101</td>\n",
       "      <td>0.289243</td>\n",
       "      <td>-0.063169</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13331</td>\n",
       "      <td>-0.048329</td>\n",
       "      <td>0.204892</td>\n",
       "      <td>-0.398776</td>\n",
       "      <td>-0.345779</td>\n",
       "      <td>-0.246285</td>\n",
       "      <td>-0.92692</td>\n",
       "      <td>0.307822</td>\n",
       "      <td>0.056094</td>\n",
       "      <td>-0.356226</td>\n",
       "      <td>-0.51482</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051965</td>\n",
       "      <td>-0.038213</td>\n",
       "      <td>-0.162514</td>\n",
       "      <td>0.041413</td>\n",
       "      <td>0.037231</td>\n",
       "      <td>-0.012559</td>\n",
       "      <td>0.018455</td>\n",
       "      <td>0.339000</td>\n",
       "      <td>0.019780</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4        5         6  \\\n",
       "13327 -0.048329  0.204892 -0.398776 -0.345779 -0.246285 -0.92692  0.307822   \n",
       "13328 -0.048329  0.204892 -0.398776 -0.345779 -0.246285 -0.92692  0.307822   \n",
       "13329 -0.048329  0.204892 -0.398776 -0.345779 -0.246285 -0.92692  0.307822   \n",
       "13330 -0.048329  0.204892 -0.398776 -0.345779 -0.246285 -0.92692  0.307822   \n",
       "13331 -0.048329  0.204892 -0.398776 -0.345779 -0.246285 -0.92692  0.307822   \n",
       "\n",
       "              7         8        9  ...       491       492       493  \\\n",
       "13327  0.056094 -0.356226 -0.51482  ...  0.015278  0.111212 -0.080474   \n",
       "13328  0.056094 -0.356226 -0.51482  ... -0.027648  0.206616  0.220193   \n",
       "13329  0.056094 -0.356226 -0.51482  ... -0.043687  0.183477  0.089562   \n",
       "13330  0.056094 -0.356226 -0.51482  ... -0.025514  0.082782 -0.084981   \n",
       "13331  0.056094 -0.356226 -0.51482  ... -0.051965 -0.038213 -0.162514   \n",
       "\n",
       "            494       495       496       497       498       499  label  \n",
       "13327 -0.012642 -0.070426  0.017351  0.011926  0.162779 -0.060195      0  \n",
       "13328 -0.081247  0.705522  0.022888 -0.010088 -0.096915 -0.005339      0  \n",
       "13329 -0.046716 -0.008383  0.064136  0.028604  0.106633  0.091731      0  \n",
       "13330 -0.113158 -0.316173  0.123388  0.005101  0.289243 -0.063169      0  \n",
       "13331  0.041413  0.037231 -0.012559  0.018455  0.339000  0.019780      0  \n",
       "\n",
       "[5 rows x 501 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = generate_data()\n",
    "print(data.isnull().values.any())\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntrain_label = train_data['label']\\ntrain_data.drop('label', axis=1, inplace = True)\\ntrain_label.reset_index(drop=True, inplace=True)\\ntrain_data.reset_index(drop=True, inplace=True)\\n\\nvalidate_label = validate_data['label']\\nvalidate_data.drop('label', axis=1, inplace = True)\\nvalidate_label.reset_index(drop=True, inplace=True)\\nvalidate_data.reset_index(drop=True, inplace=True)\\n\\nall_train = np.concatenate([train_data, validate_data])\\nall_label = np.concatenate([train_label, validate_label])\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.4)\n",
    "\n",
    "for train_index, validate_index in split.split(data, data['label']):\n",
    "    train_data = data.loc[train_index]\n",
    "    validate_data = data.loc[validate_index]\n",
    "\n",
    "'''\n",
    "train_label = train_data['label']\n",
    "train_data.drop('label', axis=1, inplace = True)\n",
    "train_label.reset_index(drop=True, inplace=True)\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "validate_label = validate_data['label']\n",
    "validate_data.drop('label', axis=1, inplace = True)\n",
    "validate_label.reset_index(drop=True, inplace=True)\n",
    "validate_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "all_train = np.concatenate([train_data, validate_data])\n",
    "all_label = np.concatenate([train_label, validate_label])\n",
    "'''\n",
    "#train_data.shape, train_label.shape, all_train.shape, all_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using MLP classifier on different title features: \n",
    "> Node2Vec, Line, Title/Body-Doc2Vec/Word2Vec/TF-IDF, TriDnr(Dim:100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataframeDataset(Dataset):\n",
    "    def __init__(self, data,  transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        index_data = self.data.iloc[index, :-1]\n",
    "        label = self.data.iloc[index, -1]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            index_data = self.transform(index_data)\n",
    "            label = self.transform(label)\n",
    "        return  index_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = dataframeDataset(train_data, transform=torch.tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(self.hidden_size, 2)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden = self.fc1(x)\n",
    "        relu = self.relu(hidden)\n",
    "        output = self.fc2(relu)\n",
    "        output = self.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1, train loss : 0.6720, valid loss : 0.6690, valid acc : 83.14%\n",
      "epoch : 2, train loss : 0.6664, valid loss : 0.6633, valid acc : 83.29%\n",
      "epoch : 3, train loss : 0.6608, valid loss : 0.6576, valid acc : 83.33%\n",
      "epoch : 4, train loss : 0.6550, valid loss : 0.6518, valid acc : 83.33%\n",
      "epoch : 5, train loss : 0.6491, valid loss : 0.6458, valid acc : 83.33%\n",
      "epoch : 6, train loss : 0.6430, valid loss : 0.6398, valid acc : 83.33%\n",
      "epoch : 7, train loss : 0.6369, valid loss : 0.6335, valid acc : 83.33%\n",
      "epoch : 8, train loss : 0.6306, valid loss : 0.6271, valid acc : 83.33%\n",
      "epoch : 9, train loss : 0.6243, valid loss : 0.6205, valid acc : 83.33%\n",
      "epoch : 10, train loss : 0.6179, valid loss : 0.6146, valid acc : 83.33%\n",
      "epoch : 11, train loss : 0.6114, valid loss : 0.6078, valid acc : 83.33%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-59c1c9230000>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mvalid_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mlocal_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_labels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Transfer to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mlocal_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/env3.6/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    282\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/lib/python3.6/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/lib/python3.6/multiprocessing/reduction.py\u001b[0m in \u001b[0;36mrecv_handle\u001b[0;34m(conn)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;34m'''Receive a handle over a local connection.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAF_UNIX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mrecvfds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mDupFd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/lib/python3.6/multiprocessing/reduction.py\u001b[0m in \u001b[0;36mrecvfds\u001b[0;34m(sock, size)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mbytes_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mancdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecvmsg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCMSG_LEN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mancdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "#cudnn.benchmark = True\n",
    "\n",
    "N, D_in, H, D_out = 64, 500, 64, 2\n",
    "\n",
    "\n",
    "params = {'batch_size': 64,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 6}\n",
    "max_epochs = 30\n",
    "\n",
    "mean_train_losses = []\n",
    "mean_valid_losses = []\n",
    "valid_acc_list = []\n",
    "\n",
    "# Load Dataframe by Dataloader\n",
    "training = dataframeDataset(train_data, transform=torch.tensor)\n",
    "validation = dataframeDataset(validate_data, transform=torch.tensor)\n",
    "training_generator = DataLoader(training, **params)\n",
    "validation_generator = DataLoader(validation, **params)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = MLP(D_in, H).to(device)\n",
    "\n",
    "# Construct our loss function and an Optimizer. Training this strange model with\n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    # Training\n",
    "    for local_batch, local_labels in training_generator:\n",
    "        # Transfer to GPU\n",
    "        local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "        # Model computations\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        y_pred = model(local_batch)\n",
    "        # Compute Loss\n",
    "        loss = criterion(y_pred.squeeze(), local_labels)\n",
    "        train_losses.append(loss.item())\n",
    "        #print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Validation\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for local_batch, local_labels in validation_generator:\n",
    "            # Transfer to GPU\n",
    "            local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "            y_pred = model(local_batch)\n",
    "            # Compute Loss\n",
    "            loss = criterion(y_pred.squeeze(), local_labels)\n",
    "            \n",
    "            valid_losses.append(loss.item())\n",
    "            \n",
    "            _, predicted = torch.max(y_pred.data, 1)\n",
    "            correct += (predicted == local_labels).sum().item()\n",
    "            total += local_labels.size(0)\n",
    "            \n",
    "    mean_train_losses.append(np.mean(train_losses))\n",
    "    mean_valid_losses.append(np.mean(valid_losses))\n",
    "    \n",
    "    accuracy = 100*correct/total\n",
    "    valid_acc_list.append(accuracy)\n",
    "    print('epoch : {}, train loss : {:.4f}, valid loss : {:.4f}, valid acc : {:.2f}%'\\\n",
    "         .format(epoch+1, np.mean(train_losses), np.mean(valid_losses), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.4)\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "for train_index, validate_index in split.split(data, data['label']):\n",
    "    train_data = data.loc[train_index]\n",
    "    validate_data = data.loc[validate_index]\n",
    "\n",
    "\n",
    "train_label = train_data['label']\n",
    "train_data.drop('label', axis=1, inplace = True)\n",
    "train_label.reset_index(drop=True, inplace=True)\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "validate_label = validate_data['label']\n",
    "validate_data.drop('label', axis=1, inplace = True)\n",
    "validate_label.reset_index(drop=True, inplace=True)\n",
    "validate_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "all_train = np.concatenate([train_data, validate_data])\n",
    "all_label = np.concatenate([train_label, validate_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8       1\n",
       "10      1\n",
       "13      1\n",
       "16      1\n",
       "17      1\n",
       "       ..\n",
       "7945    1\n",
       "7968    1\n",
       "7977    1\n",
       "7980    1\n",
       "7982    1\n",
       "Name: label, Length: 1333, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label[train_label==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333020813800862"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(C=1.0, kernel='rbf', gamma=0.1)\n",
    "clf.fit(train_data, train_label)\n",
    "clf.score(train_data, train_label)\n",
    "clf.score(validate_data, validate_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using tensorflow 2-hidden layer mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_model_on_train_set(D_in=500, n_neurons_1=150, n_neurons_2 = 70, learning_rate = 0.01, n_epochs = 30, batch_size = 50):\n",
    "    # here we build a two layers NN model and test on validation set, you may improve it to a CV version\n",
    "    # n_neurons_1 : number of neurons in the first layer\n",
    "    # n_neurons_2  : number of neurons in the second layer\n",
    "    # learning_rate : the learning rate of BGD\n",
    "    # n_epochs : times of training the model\n",
    "    # batch_size : since we adopted BGD, then we need to define the size of a size\n",
    "    # initialize variables\n",
    "    X = tf.placeholder(tf.float32, shape=(None, D_in), name='X')\n",
    "    y = tf.placeholder(tf.int64, shape=(None), name = 'y')\n",
    "\n",
    "    # weights\n",
    "    W1 = tf.Variable(tf.truncated_normal((D_in, n_neurons_1),stddev = 0.01), name = 'layer_1')\n",
    "    W2 = tf.Variable(tf.truncated_normal((n_neurons_1, n_neurons_2),stddev = 0.01), name = 'layer_2')\n",
    "    W3 = tf.Variable(tf.truncated_normal((n_neurons_2 , 2),stddev = 0.01), name = 'output_layer')\n",
    "\n",
    "    # biases\n",
    "    b1 = tf.Variable(tf.zeros([n_neurons_1]), name='b_1')\n",
    "    b2 = tf.Variable(tf.zeros([n_neurons_2]), name='b_2')\n",
    "    b3 = tf.Variable(tf.zeros([2]), name='b_3')\n",
    "\n",
    "    # the output of each layer\n",
    "    Z1 = tf.nn.relu(tf.matmul(X,W1) + b1)\n",
    "    Z2 = tf.nn.relu(tf.matmul(Z1, W2) + b2)\n",
    "    output = tf.matmul(Z2, W3) + b3\n",
    "\n",
    "    # define loss function. Cross-entropy was adopted rather than MSE\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = output)\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "    # define accuracy\n",
    "    correct = tf.nn.in_top_k(output, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    # run everything\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            for iteration in range(len(train_data) // batch_size):\n",
    "                # 因为要batchSize=50个50个的取，取到末尾时可能不够，所以用个if判定一下\n",
    "                if (iteration + 1) * batch_size <= len(train_data):\n",
    "                    X_batch = np.array(train_data[iteration * batch_size : iteration * batch_size + batch_size])\n",
    "                    y_batch = np.array(train_label[iteration * batch_size : iteration * batch_size + batch_size])\n",
    "                else:\n",
    "                    X_batch = np.array(train_data[iteration * batch_size : ])\n",
    "                    y_batch = np.array(train_label[iteration * batch_size : ])\n",
    "                sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "            # train error\n",
    "            acc_train = accuracy.eval(feed_dict={X:X_batch, y:y_batch})\n",
    "            # test error\n",
    "            acc_test = accuracy.eval(feed_dict={X:np.array(validate_data),\n",
    "                                               y:np.array(validate_label)})\n",
    "            print(epoch, 'Train accuracy:', acc_train, 'Test accuracy:', acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.84 Test accuracy: 0.8333021\n",
      "1 Train accuracy: 0.84 Test accuracy: 0.8333021\n",
      "2 Train accuracy: 0.84 Test accuracy: 0.8333021\n",
      "3 Train accuracy: 0.84 Test accuracy: 0.8333021\n",
      "4 Train accuracy: 0.84 Test accuracy: 0.8333021\n",
      "5 Train accuracy: 0.84 Test accuracy: 0.8333021\n",
      "6 Train accuracy: 0.84 Test accuracy: 0.8333021\n",
      "7 Train accuracy: 0.84 Test accuracy: 0.8333021\n",
      "8 Train accuracy: 0.84 Test accuracy: 0.8333021\n",
      "9 Train accuracy: 0.84 Test accuracy: 0.8333021\n",
      "10 Train accuracy: 0.84 Test accuracy: 0.8333021\n",
      "11 Train accuracy: 0.84 Test accuracy: 0.8333021\n",
      "12 Train accuracy: 0.84 Test accuracy: 0.8333021\n",
      "13 Train accuracy: 0.84 Test accuracy: 0.8333021\n",
      "14 Train accuracy: 0.84 Test accuracy: 0.8333021\n",
      "15 Train accuracy: 0.84 Test accuracy: 0.8333021\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-a4a4785169e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mNN_model_on_train_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-45-8ba2f3fb7368>\u001b[0m in \u001b[0;36mNN_model_on_train_set\u001b[0;34m(D_in, n_neurons_1, n_neurons_2, learning_rate, n_epochs, batch_size)\u001b[0m\n\u001b[1;32m     50\u001b[0m                     \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0;31m# train error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0macc_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/env3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/env3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/env3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/env3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/env3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/env3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NN_model_on_train_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
